




# TensorFlow XLA

- モデルを高速化するためのバックエンド。    
- バックエンドとしてLLVMを採用。高速化したモデルをLLVMを通して色んなプラットフォームに展開できる。    


# 参照    

- 一覧    
https://qiita.com/bonotake/items/cbd44abbcbe333f264d8     

- ブログ    
https://blogs.yahoo.co.jp/verification_engineer/71526428.html    
https://blogs.yahoo.co.jp/verification_engineer/71502790.html    

- XLA Overview    
https://www.tensorflow.org/performance/xla/    


- mnist softmax xla    
https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py    

- High-performance TensorFlow* on Intel® Xeon® Using nGraph    
https://ai.intel.com/high-performance-tensorflow-on-intel-xeon-using-ngraph/    

- llvm    
一応知っといた方がいいなぁと。    
https://itchyny.hatenablog.com/entry/2017/02/27/100000    
